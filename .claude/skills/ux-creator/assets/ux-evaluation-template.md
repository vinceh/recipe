# UX Evaluation Report: [Product/Feature Name]

**Date:** [Evaluation date]
**Evaluator(s):** [Names]
**Scope:** [What was evaluated - specific feature, page, flow, or entire product]
**Methodology:** [Heuristic evaluation, cognitive walkthrough, or combined]

---

## Executive Summary

**Overall UX Score:** [X/10] ([Excellent/Good/Fair/Poor])

**Total Issues Found:**
- **Critical:** [X] issues (Must fix)
- **Major:** [X] issues (Should fix)
- **Minor:** [X] issues (Nice to fix)

**Key Findings:**
[1-3 sentences summarizing most important findings and overall UX state]

**Recommendation:**
[High-level recommendation: Ready for launch / Address critical issues first / Needs significant UX improvements]

---

## Critical Issues (Must Fix)

### Issue #1: [Brief title]

**Heuristic Violated:** #[Number] [Name of heuristic]
**Location:** [Where in the interface - page, screen, specific element]
**Severity:** Critical

**Description:**
[Detailed description of the problem]

**User Impact:**
[How this affects users - percentage impacted, frequency, consequences]

**Current Behavior:**
- [What happens now]
- [Why this is problematic]

**Recommended Fix:**
1. [Specific action to take]
2. [Additional steps if needed]
3. [Validation criteria]

**Screenshots/Evidence:**
[Attach or describe visual evidence]

---

### Issue #2: [Brief title]

[Repeat structure above for each critical issue]

---

## Major Issues (Should Fix)

### Issue #3: [Brief title]

**Heuristic Violated:** #[Number] [Name of heuristic]
**Location:** [Where in the interface]
**Severity:** Major

**Description:**
[What's wrong]

**User Impact:**
[How users are affected]

**Recommended Fix:**
[How to fix]

---

[Repeat for each major issue]

---

## Minor Issues (Nice to Fix)

| # | Issue | Location | Heuristic | Recommendation |
|---|-------|----------|-----------|----------------|
| 1 | [Brief description] | [Page/element] | #[X] | [Quick fix] |
| 2 | [Brief description] | [Page/element] | #[X] | [Quick fix] |
| 3 | [Brief description] | [Page/element] | #[X] | [Quick fix] |

---

## Issues by Heuristic

Summary of violations grouped by Nielsen's 10 heuristics:

**#1 Visibility of System Status:** [X] issues
- [Brief list of issues]

**#2 Match Between System and Real World:** [X] issues
- [Brief list]

**#3 User Control and Freedom:** [X] issues
- [Brief list]

**#4 Consistency and Standards:** [X] issues
- [Brief list]

**#5 Error Prevention:** [X] issues
- [Brief list]

**#6 Recognition Rather Than Recall:** [X] issues
- [Brief list]

**#7 Flexibility and Efficiency of Use:** [X] issues
- [Brief list]

**#8 Aesthetic and Minimalist Design:** [X] issues
- [Brief list]

**#9 Help Users Recognize, Diagnose, and Recover from Errors:** [X] issues
- [Brief list]

**#10 Help and Documentation:** [X] issues
- [Brief list]

---

## Accessibility Findings (WCAG 2.2 AA)

**Overall Accessibility Score:** [Pass/Fail/Partial]

**Critical Accessibility Issues:**
- [Issue 1 with WCAG criterion violated]
- [Issue 2 with WCAG criterion violated]

**Accessibility Improvements Needed:**
- [Improvement 1]
- [Improvement 2]

---

## Cognitive Walkthrough Results

[Include this section if cognitive walkthrough was performed]

### Task 1: [Task name]

**Success Rate:** [X/Y evaluators completed successfully]
**Average Steps:** [X] (Expected: [Y])

**Issues Found:**
1. **Step [X]**: [Problem with what users will think/do]
   - Fix: [Recommendation]
2. **Step [Y]**: [Problem]
   - Fix: [Recommendation]

---

### Task 2: [Task name]

[Repeat structure]

---

## Positive Observations

What's working well:
- [Good UX practice observed]
- [Another strength]
- [More positive findings]

---

## Prioritization and Roadmap

### Immediate Fixes (Critical - Before Launch)

| Issue | Impact | Effort | Priority |
|-------|--------|--------|----------|
| [Issue #1] | High | Low | P0 |
| [Issue #2] | High | High | P0 |

**Estimated Time:** [X days/weeks]

### Short-Term Fixes (Major - Next Sprint)

| Issue | Impact | Effort | Priority |
|-------|--------|--------|----------|
| [Issue #3] | High | Low | P1 |
| [Issue #4] | Medium | Low | P1 |

**Estimated Time:** [X days/weeks]

### Long-Term Improvements (Minor - Backlog)

| Issue | Impact | Effort | Priority |
|-------|--------|--------|----------|
| [Issue #5] | Low | Low | P3 |
| [Issue #6] | Low | Medium | P3 |

**Estimated Time:** [X days/weeks]

---

## Success Metrics

How to measure UX improvement after fixes are implemented:

**Quantitative Metrics:**
- Task completion rate: [Current X%] → [Target Y%]
- Time to complete task: [Current X seconds] → [Target Y seconds]
- Error rate: [Current X%] → [Target Y%]
- User satisfaction score: [Current X/10] → [Target Y/10]

**Qualitative Metrics:**
- Support ticket reduction: [Current X/week] → [Target Y/week]
- User feedback sentiment: [Current state] → [Target state]

---

## Recommendations Summary

### Quick Wins (High Impact, Low Effort)
1. [Recommendation 1]
2. [Recommendation 2]
3. [Recommendation 3]

### Major Projects (High Impact, High Effort)
1. [Recommendation 1]
2. [Recommendation 2]

### Consider Not Doing (Low Impact, High Effort)
1. [Issue that may not be worth fixing]

---

## Methodology Details

**Heuristic Evaluation:**
- Number of evaluators: [X]
- Heuristics used: Nielsen's 10 Usability Heuristics
- Scope: [Pages/features evaluated]
- Duration: [Hours]

**Cognitive Walkthrough:**
- Tasks evaluated: [X]
- Evaluators: [X]
- User persona: [Description]
- Duration: [Hours]

**Tools Used:**
- [Accessibility checker tool]
- [Analytics platform]
- [Screen reader testing]
- [Other tools]

---

## Appendix

### A. Severity Rating Scale

**Critical (P0):**
- Blocks users from completing core tasks
- Causes data loss or security issues
- Affects majority of users
- No workaround exists

**Major (P1):**
- Significantly frustrates users
- Impacts efficiency or satisfaction
- Workaround exists but not obvious
- Affects substantial portion of users

**Minor (P2):**
- Small annoyances
- Minimal impact on completion
- Affects edge cases or few users
- Clear workaround available

### B. Nielsen's 10 Heuristics Reference

1. **Visibility of system status**: Keep users informed about what's happening
2. **Match between system and real world**: Use familiar language and concepts
3. **User control and freedom**: Provide undo/redo, clear exits
4. **Consistency and standards**: Follow platform conventions
5. **Error prevention**: Prevent errors before they occur
6. **Recognition rather than recall**: Make options visible
7. **Flexibility and efficiency of use**: Support both novice and expert users
8. **Aesthetic and minimalist design**: Remove unnecessary elements
9. **Help users recognize, diagnose, and recover from errors**: Clear error messages with solutions
10. **Help and documentation**: Provide contextual help when needed

### C. Contact Information

For questions about this evaluation, contact:
- [Name]
- [Email]
- [Date]
